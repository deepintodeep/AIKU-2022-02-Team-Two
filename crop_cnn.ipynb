{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDS3O3m-D2ql"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Pd_pRTGkI8k"
      },
      "outputs": [],
      "source": [
        "# f12 -> console\n",
        "\n",
        "# function ClickConnect(){\n",
        "#     console.log(\"prevent colab disconnecting...\");\n",
        "#     document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "# }\n",
        "# undefined\n",
        "# setInterval(ClickConnect, 60*5000) // 5 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVpk5TyzyDhQ",
        "outputId": "bda2e4b4-b614-47c0-e990-adb8659f7251"
      },
      "outputs": [],
      "source": [
        "!pip install einops\n",
        "!pip install timm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOWDMN5Vi8s3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from einops import rearrange # for changing channels\n",
        "\n",
        "from glob import glob\n",
        "import json\n",
        "import csv\n",
        "import cv2 # for image load\n",
        "import albumentations as A\n",
        "import albumentations.pytorch\n",
        "\n",
        "import plotly.express as px # for graph\n",
        "\n",
        "import timm # for pretrained models\n",
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "\n",
        "from sklearn.metrics import f1_score # for f1 score\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhqUD5C6212y"
      },
      "source": [
        "# WanDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWYBAK2ZQFuZ",
        "outputId": "cd5d8c00-09c9-4bcb-f38f-6f7ea90fc57e"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhruByp_MtYl",
        "outputId": "d46c7aca-aa6b-4043-a802-fa17d5b39659"
      },
      "outputs": [],
      "source": [
        "# Get Test Dataset from Google Drive. (Approximately 3m)\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/data\" -d \"/content/\"\n",
        "!unzip \"/content/train\" -d \"/content/\"\n",
        "!unzip \"/content/test\" -d \"/content/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvueExrDD76H"
      },
      "source": [
        "# ENV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYmPNhPQxuSs",
        "outputId": "e3c06e4a-5b02-40a6-db1c-7ffa49e94c66"
      },
      "outputs": [],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "model_list = {\n",
        "    \"resnet50\" : {\n",
        "        \"model\" : \"resnet50\", \n",
        "        \"input_size\" : (224, 224),\n",
        "        \"classifier_in_feature\" : 2048\n",
        "        }, \n",
        "    \"efficientnet_b0\" : {\n",
        "        \"model\" : \"efficientnet_b0\", \n",
        "        \"input_size\" : (224, 224),\n",
        "        \"classifier_in_feature\" : 1280\n",
        "        }, \n",
        "    \"resnet18\" : {\n",
        "        \"model\" : \"resnet18\", \n",
        "        \"input_size\" : (224, 224),\n",
        "        \"classifier_in_feature\" : 1024\n",
        "        }, \n",
        "    \"resnet50\" : {\n",
        "        \"model\" : \"resnet50\", \n",
        "        \"input_size\" : (224, 224),\n",
        "        \"classifier_in_feature\" : 1024\n",
        "        }, \n",
        "    \"convnext_base_384_in22ft1k\" : {\n",
        "        \"model\" : \"convnext_base_384_in22ft1k\", \n",
        "        \"input_size\" : (384, 384),\n",
        "        \"classifier_in_feature\" : 1024\n",
        "        }\n",
        "}\n",
        "\n",
        "optimizer_list = [\"adam\", \"sgd\"]\n",
        "\n",
        "model_name = \"convnext_base_384_in22ft1k\"\n",
        "\n",
        "CONFIG = {\n",
        "    \"batch_size\" : 16,\n",
        "    \"epoch\" : 200,\n",
        "    \"learning_rate\" : 1e-4,\n",
        "    \"input_size\" : model_list[model_name][\"input_size\"],\n",
        "    \"backbone\" : model_list[model_name][\"model\"],\n",
        "    \"classifier_in_feature\": model_list[model_name][\"classifier_in_feature\"],\n",
        "    \"device\" : device,\n",
        "    \"patience\" : 10,\n",
        "    \"optimizer\" : optimizer_list[0],\n",
        "    \"input_norm_mean\" : IMAGENET_DEFAULT_MEAN,\n",
        "    \"input_norm_std\" : IMAGENET_DEFAULT_STD\n",
        "}\n",
        "\n",
        "batch_size = CONFIG[\"batch_size\"]\n",
        "epochs = CONFIG[\"epoch\"]\n",
        "learning_rate = CONFIG[\"learning_rate\"]\n",
        "input_size = CONFIG[\"input_size\"]\n",
        "backbone = CONFIG[\"backbone\"]\n",
        "classifier_in_feature = CONFIG[\"classifier_in_feature\"]\n",
        "device = CONFIG[\"device\"]\n",
        "patience = CONFIG[\"patience\"]\n",
        "optimizer = CONFIG[\"optimizer\"]\n",
        "input_norm_mean = CONFIG[\"input_norm_mean\"]\n",
        "input_norm_std = CONFIG[\"input_norm_std\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0JhaPGEztdy"
      },
      "source": [
        "# Read the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFqAbN1PjNNw"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, json_dir_list, img_dir_list, feature_label, transform=None):\n",
        "        self.json_dir_list = json_dir_list\n",
        "        self.img_dir_list = img_dir_list\n",
        "        self.transform = transform\n",
        "        self.feature_label = feature_label # dictionary\n",
        "        self.img_list = []\n",
        "        \n",
        "        for i in range(len(json_dir_list)):\n",
        "            json_path, img_path = json_dir_list[i], img_dir_list[i]\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            if self.transform:\n",
        "                image = self.transform(image=image)['image']\n",
        "\n",
        "            label = 0\n",
        "            with open(json_path, \"r\") as j:\n",
        "                i = json.load(j)\n",
        "                feature = f\"{i['annotations']['crop']}_{i['annotations']['disease']}_{i['annotations']['risk']}\"\n",
        "                label = self.feature_label[feature] # label : int\n",
        "\n",
        "            self.img_list.append((image, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_dir_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.img_list[idx]\n",
        "\n",
        "        return image, label #label type: int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iji8UH2uyvYL"
      },
      "outputs": [],
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, img_dir_list, transform=None):\n",
        "        self.img_dir_list = img_dir_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_dir_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.img_dir_list[index]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)['image']\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFlGvATgObr-"
      },
      "outputs": [],
      "source": [
        "# WANDB_TRAIN_DATA_PATH = train_artifact_dir\n",
        "# WANDB_TEST_DATA_PATH = test_artifact_dir\n",
        "COLAB_TRAIN_DATA_PATH = \"/content\"\n",
        "COLAB_TEST_DATA_PATH = \"/content/test\"\n",
        "# LOCAL_TRAIN_DATA_PATH = \"./crop_data/train.csv\"\n",
        "\n",
        "TRAIN_DATA_PATH = COLAB_TRAIN_DATA_PATH\n",
        "TEST_DATA_PATH = COLAB_TEST_DATA_PATH\n",
        "Possible_comb_list = []\n",
        "TRAIN_JSON_LIST, TRAIN_JPG_LIST = [], []\n",
        "\n",
        "# Get Train Data\n",
        "\n",
        "with open(TRAIN_DATA_PATH+\"/train.csv\", \"r\") as train_csv:\n",
        "    i = csv.reader(train_csv, delimiter=',')\n",
        "    for j in i:\n",
        "        Possible_comb_list.append(j[1])\n",
        "\n",
        "Possible_comb_list = list(set(Possible_comb_list[1:])) # length = 25\n",
        "label_feature = {i: Possible_comb_list[i] for i in range(len(Possible_comb_list))} # dictionary\n",
        "feature_label = {Possible_comb_list[i]: i for i in range(len(Possible_comb_list))} # dictionary\n",
        "\n",
        "with open(TRAIN_DATA_PATH+\"/train.csv\", \"r\") as f:\n",
        "    i = csv.reader(f, delimiter=',')\n",
        "    for j in i:\n",
        "        TRAIN_JSON_LIST.append(TRAIN_DATA_PATH+f\"/train/{j[0]}/{j[0]}.json\")\n",
        "        TRAIN_JPG_LIST.append(TRAIN_DATA_PATH+f\"/train/{j[0]}/{j[0]}.jpg\")\n",
        "TRAIN_JSON_LIST = TRAIN_JSON_LIST[1:]\n",
        "TRAIN_JPG_LIST = TRAIN_JPG_LIST[1:]\n",
        "\n",
        "# Get Test Data\n",
        "\n",
        "TEST_JPG_LIST = []\n",
        "with open(TRAIN_DATA_PATH+\"/sample_submission.csv\", \"r\") as f:\n",
        "    i = csv.reader(f, delimiter=',')\n",
        "    \n",
        "    for j in i:\n",
        "        TEST_JPG_LIST.append(TEST_DATA_PATH+f\"/{j[0]}/{j[0]}.jpg\")\n",
        "TEST_JPG_LIST = TEST_JPG_LIST[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8wi6_Lp7lnx"
      },
      "outputs": [],
      "source": [
        "A_transform = A.Compose([\n",
        "    A.Resize(input_size[0], input_size[1]),\n",
        "    albumentations.augmentations.transforms.Normalize(\n",
        "        mean=input_norm_mean, \n",
        "        std=input_norm_std\n",
        "        ),\n",
        "    albumentations.pytorch.transforms.ToTensorV2()\n",
        "])\n",
        "\n",
        "# numpy : h w c\n",
        "# torch : c h w\n",
        "\n",
        "labeled_dataset = CustomDataset(TRAIN_JSON_LIST, TRAIN_JPG_LIST, feature_label, transform=A_transform)\n",
        "test_dataset = TestDataset(TEST_JPG_LIST, A_transform)\n",
        "\n",
        "def dataset_split(labeled_dataset, ratio):\n",
        "    train_size = int(ratio * len(labeled_dataset))\n",
        "    test_size = len(labeled_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(labeled_dataset, [train_size, test_size])\n",
        "    return train_dataset, val_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dxbrQ3WcaYF"
      },
      "source": [
        "# Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fwkbFWMcaHl"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.best_score = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}.\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        if self.verbose:\n",
        "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...\")\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MBMeTCDzyPD"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDcKsOYRxuSx"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, backbone, classifier_in_feature, pretrained=True):\n",
        "        super(Model, self).__init__()\n",
        "        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n",
        "        self.backbone.reset_classifier(0)\n",
        "        self.classifier = nn.Linear(\n",
        "                in_features=classifier_in_feature,\n",
        "                out_features=25\n",
        "            )\n",
        "        \n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad=False\n",
        "\n",
        "    def forward(self, images):\n",
        "        output = self.backbone(images) # bs 1 1000\n",
        "        output = self.classifier(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Model(backbone, classifier_in_feature, pretrained=True)\n",
        "model = model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "if optimizer == \"adam\":\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
        "elif optimizer == \"sgd\":\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynj_rMVhzzTQ"
      },
      "source": [
        "# Training & Validation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgSLW0c6xuSx"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, loss_fn, dataloader, device):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    running_loss = 0\n",
        "    train_pred, train_y = [], []\n",
        "\n",
        "    for (x, y) in enumerate(tqdm(dataloader)):\n",
        "        x, y = x.to(device), y.to(device) # x shape: 8 3 224 224\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss\n",
        "        correct += torch.sum(pred.argmax(dim=1) == y)\n",
        "        train_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "        train_y += y.detach().cpu().numpy().tolist()\n",
        "\n",
        "    score = f1_score(train_y, train_pred, average='macro')\n",
        "\n",
        "    acc = 100 * correct / (len(dataloader) * batch_size)\n",
        "    running_loss = running_loss/len(dataloader)\n",
        "    return running_loss, acc, score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rtx8mlTIxuSy"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def valid_one_epoch(model, dataloader, device):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    val_pred, val_y = [], []\n",
        "\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y).item()\n",
        "\n",
        "        running_loss += loss\n",
        "        correct += torch.sum(pred.argmax(dim=1) == y)\n",
        "        val_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "        val_y += y.detach().cpu().numpy().tolist()\n",
        "\n",
        "    score = f1_score(val_y, val_pred, average='macro')\n",
        "        \n",
        "    acc = 100 * correct / (len(dataloader) * batch_size)\n",
        "    running_loss = running_loss/len(dataloader)\n",
        "\n",
        "    return running_loss, acc, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF9RW1TIwQe_"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test(model, dataloader, device, result_csv):\n",
        "    model.eval()\n",
        "    rst = []\n",
        "    for batch, x in enumerate(tqdm(dataloader)):\n",
        "        x = x.to(device)\n",
        "        pred = model(x)\n",
        "        estimation = pred.argmax(dim=1)\n",
        "\n",
        "        df = pd.read_csv(result_csv)\n",
        "        for i in range(len(x)):\n",
        "            rst.append((batch * len(x) + i, label_feature[estimation[i].item()]))\n",
        "            df.loc[batch * len(x) + i, 'label'] = label_feature[estimation[i].item()]\n",
        "        df.to_csv(result_csv, index=False)\n",
        "    \n",
        "    return rst\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilhFXFpH0KIK"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCpSP4h2ha0n"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "            project=\"crop_data\",\n",
        "            config=CONFIG\n",
        "            ) # start a new run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5sgmb3vxuSy"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"best_model.pt\"\n",
        "epoch_loss_list = []\n",
        "train_loss_list = []\n",
        "cur_f1_max = 0\n",
        "\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=False)\n",
        "\n",
        "for e in range(epochs):\n",
        "    print(f\"\\nEpoch {e+1}\")\n",
        "    train_dataset, val_dataset = dataset_split(labeled_dataset, 0.8) # 8 : 2 = train : val\n",
        "    train_dataloader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "    val_dataloader = DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    train_loss, train_acc, train_score = train_one_epoch(model, optimizer, loss_fn, train_dataloader, device)\n",
        "    val_loss, val_acc, val_score = valid_one_epoch(model, val_dataloader, device)\n",
        "\n",
        "    wandb.log({\"train loss\": train_loss, \n",
        "               \"train accuracy\": train_acc, \n",
        "               \"validation loss\": val_loss, \n",
        "               \"validation accuracy\": val_acc, \n",
        "               \"train f1_score\": train_score, \n",
        "               \"validation f1_score\": val_score})\n",
        "\n",
        "    early_stopping(val_loss, model)\n",
        "    print(f\"train f1_score: {train_score}\\tvalidation f1_score: {val_score}\")\n",
        "\n",
        "    if val_score > cur_f1_max:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        print(f\"Validation f1_score increased ({cur_f1_max:.6f} --> {val_score:.6f}). Saving model ...\")\n",
        "        cur_f1_max = val_score\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "    epoch_loss_list.append(val_loss)\n",
        "    train_loss_list.append(train_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjH1PZGhDbtB"
      },
      "outputs": [],
      "source": [
        "# wandb.save(MODEL_PATH)\n",
        "\n",
        "artifact = wandb.Artifact('model', type='model')\n",
        "artifact.add_file(MODEL_PATH)\n",
        "run.log_artifact(artifact)\n",
        "\n",
        "wandb.finish() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05w4PMXmwpvP"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skMb8Y5Kwpd6"
      },
      "outputs": [],
      "source": [
        "# # Get best Model\n",
        "\n",
        "# run = wandb.init(project='crop_data')\n",
        "# artifact = run.use_artifact('model:latest', type='model')\n",
        "# artifact_dir = artifact.download()\n",
        "# wandb.finish()\n",
        "# MODEL_PATH = artifact_dir\n",
        "\n",
        "model.load_state_dict(torch.load(MODEL_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsJcAXG0wtkm"
      },
      "outputs": [],
      "source": [
        "# Testing\n",
        "\n",
        "print(f\"Now Testing\")\n",
        "    \n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "result = test(model, test_dataloader, device, TRAIN_DATA_PATH+\"/sample_submission.csv\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX3qgsrqNXJx"
      },
      "outputs": [],
      "source": [
        "# Uploading data to Wandb\n",
        "\n",
        "upload = wandb.init(project=\"crop_data\", name=\"uploading_result\", config=CONFIG)\n",
        "\n",
        "artifact = wandb.Artifact('crop_data_result', type='dataset')\n",
        "artifact.add_file(TRAIN_DATA_PATH+\"/sample_submission.csv\")\n",
        "upload.log_artifact(artifact)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8f7ZY3psqtp"
      },
      "source": [
        "# Train/Test Multiple time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8jjSJoRstal"
      },
      "outputs": [],
      "source": [
        "model_list = [\n",
        "            #   (\"resnet50\", 2048), \n",
        "            #   (\"efficientnet_b0\", 1280), \n",
        "            #   (\"resnet18\", 512), \n",
        "            #   (\"swin_base_patch4_window7_224_in22k\", 1024),\n",
        "            #   (\"convnext_base_384_in22ft1k\", 1024)\n",
        "]\n",
        "\n",
        "for i in range(len(model_list)):\n",
        "    CONFIG[\"backbone\"] = model_list[i][0]\n",
        "    CONFIG[\"classifier_in_feature\"] = model_list[i][1]\n",
        "    backbone = CONFIG[\"backbone\"]\n",
        "    classifier_in_feature = CONFIG[\"classifier_in_feature\"]\n",
        "\n",
        "    model = Model(backbone, classifier_in_feature, pretrained=True)\n",
        "    model = model.to(device)\n",
        "\n",
        "    run = wandb.init(\n",
        "            project=\"crop_data\",\n",
        "            config=CONFIG\n",
        "            ) # start a new run\n",
        "\n",
        "    MODEL_PATH = \"best_model.pt\"\n",
        "    cur_f1_max = 0\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=False)\n",
        "\n",
        "    epoch_loss_list, train_loss_list = [], []\n",
        "\n",
        "    for e in range(epochs):\n",
        "        print(f\"\\nEpoch {e+1}\")\n",
        "        train_dataset, val_dataset = dataset_split(labeled_dataset, 0.8) # 8 : 2 = train : val\n",
        "        train_dataloader = DataLoader(\n",
        "            dataset=train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "        val_dataloader = DataLoader(\n",
        "            dataset=val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        train_loss, train_acc, train_score = train_one_epoch(model, optimizer, loss_fn, train_dataloader, device)\n",
        "        val_loss, val_acc, val_score = valid_one_epoch(model, val_dataloader, device)\n",
        "\n",
        "        wandb.log({\"train loss\": train_loss, \n",
        "                \"train accuracy\": train_acc, \n",
        "                \"validation loss\": val_loss, \n",
        "                \"validation accuracy\": val_acc, \n",
        "                \"train f1_score\": train_score, \n",
        "                \"validation f1_score\": val_score})\n",
        "\n",
        "        early_stopping(val_loss, model)\n",
        "        print(f\"train f1_score: {train_score}\\tvalidation f1_score: {val_score}\")\n",
        "\n",
        "        if val_score > cur_f1_max:\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            print(f\"Validation f1_score increased ({cur_f1_max:.6f} --> {val_score:.6f}). Saving model ...\")\n",
        "            cur_f1_max = val_score\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        epoch_loss_list.append(val_loss)\n",
        "        train_loss_list.append(train_loss)\n",
        "        # wandb.save(MODEL_PATH)\n",
        "\n",
        "    artifact = wandb.Artifact('model', type='model')\n",
        "    artifact.add_file(MODEL_PATH)\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    # wandb.finish()\n",
        "\n",
        "    # # Get best Model\n",
        "\n",
        "    # run = wandb.init(project='crop_data')\n",
        "    # artifact = run.use_artifact('model:latest', type='model')\n",
        "    # artifact_dir = artifact.download()\n",
        "    # wandb.finish()\n",
        "    # MODEL_PATH = artifact_dir\n",
        "\n",
        "    model.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "    print(f\"Now Testing\")\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    result = test(model, test_dataloader, device, TRAIN_DATA_PATH+\"/sample_submission.csv\")\n",
        "\n",
        "    print(result)\n",
        "\n",
        "\n",
        "    # Uploading data to Wandb\n",
        "\n",
        "    # upload = wandb.init(project=\"crop_data\")\n",
        "    artifact = wandb.Artifact('crop_data_result', type='dataset')\n",
        "    artifact.add_file(TRAIN_DATA_PATH+\"/sample_submission.csv\")\n",
        "    upload.log_artifact(artifact)\n",
        "\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6vvdJZRN7aT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
